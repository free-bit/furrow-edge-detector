{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports & Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T13:10:36.234610Z",
     "start_time": "2021-04-20T13:10:36.230983Z"
    }
   },
   "outputs": [],
   "source": [
    "# Move to the root\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "if os.path.basename(cwd) != \"cv-in-farming\":\n",
    "    os.chdir(\"../\")\n",
    "print(\"Current directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T13:10:45.688677Z",
     "start_time": "2021-04-20T13:10:36.734826Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "\n",
    "from src.dataloader import FurrowDataset\n",
    "from src.model import RidgeDetector\n",
    "from src.solver import Solver, save_checkpoint, load_checkpoint, prepare_batch_visualization\n",
    "from utils.helpers import show_image\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T13:10:51.249691Z",
     "start_time": "2021-04-20T13:10:46.646318Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Input: Input format for the network. (Allowed formats: darr, rgb, drgb, rgb-darr, rgb-drgb)\n",
    "input_format = \"darr\"\n",
    "\n",
    "crop_down = True # Crop from 120px left, 80px down with size 400x400\n",
    "normalize = True # With respect to ImageNet mean and variation\n",
    "\n",
    "# Input: Number of frames to consider from each folder\n",
    "max_frames = 1000\n",
    "\n",
    "# Input: Configure how each folder should be loaded for training, validation and test.\n",
    "\n",
    "# Training dataset configuration\n",
    "train_data_args0 = {\n",
    "    \"data_path\": \"dataset/train/20201112_125754\", # November capture\n",
    "    \"crop_down\": crop_down,\n",
    "    \"normalize\": normalize,\n",
    "    \"input_format\": input_format,\n",
    "    \"load_edge\": True,\n",
    "    \"allow_missing_files\": False,\n",
    "    \"edge_width\": 3,\n",
    "    \"load_time\": False,\n",
    "    \"start\": 0,\n",
    "    \"max_frames\": max_frames,\n",
    "}\n",
    "\n",
    "train_data_args1 = {\n",
    "    \"data_path\": \"dataset/train/20201112_131032\", # November capture\n",
    "    \"crop_down\": crop_down,\n",
    "    \"normalize\": normalize,\n",
    "    \"input_format\": input_format,\n",
    "    \"load_edge\": True,\n",
    "    \"allow_missing_files\": False,\n",
    "    \"edge_width\": 3,\n",
    "    \"load_time\": False,\n",
    "    \"start\": 0,\n",
    "    \"max_frames\": max_frames,\n",
    "}\n",
    "\n",
    "train_data_args2 = {\n",
    "    \"data_path\": \"dataset/train/20201112_131702\", # November capture\n",
    "    \"crop_down\": crop_down,\n",
    "    \"normalize\": normalize,\n",
    "    \"input_format\": input_format,\n",
    "    \"load_edge\": True,\n",
    "    \"allow_missing_files\": False,\n",
    "    \"edge_width\": 3,\n",
    "    \"load_time\": False,\n",
    "    \"start\": 0,\n",
    "    \"max_frames\": max_frames,\n",
    "}\n",
    "\n",
    "train_data_args3 = {\n",
    "    \"data_path\": \"dataset/train/20201112_140127\", # November capture\n",
    "    \"crop_down\": crop_down,\n",
    "    \"normalize\": normalize,\n",
    "    \"input_format\": input_format,\n",
    "    \"allow_missing_files\": False,\n",
    "    \"load_edge\": True,\n",
    "    \"edge_width\": 3,\n",
    "    \"load_time\": False,\n",
    "    \"start\": 0,\n",
    "    \"max_frames\": max_frames,\n",
    "}\n",
    "\n",
    "train_data_args4 = {\n",
    "    \"data_path\": \"dataset/train/20201112_140726\", # November capture\n",
    "    \"crop_down\": crop_down,\n",
    "    \"normalize\": normalize,\n",
    "    \"input_format\": input_format,\n",
    "    \"load_edge\": True,\n",
    "    \"allow_missing_files\": True,\n",
    "    \"edge_width\": 3,\n",
    "    \"load_time\": False,\n",
    "    \"start\": 0,\n",
    "    \"max_frames\": max_frames,\n",
    "}\n",
    "\n",
    "train_data_args5 = {\n",
    "    \"data_path\": \"dataset/train/20201112_125754_aug\",\n",
    "    \"crop_down\": crop_down,\n",
    "    \"normalize\": normalize,\n",
    "    \"input_format\": input_format,\n",
    "    \"load_edge\": True,\n",
    "    \"allow_missing_files\": False,\n",
    "    \"edge_width\": 3,\n",
    "    \"load_time\": False,\n",
    "#     \"max_frames\": max_frames,\n",
    "}\n",
    "\n",
    "train_data_args6 = {\n",
    "    \"data_path\": \"dataset/train/20201112_131032_aug\",\n",
    "    \"crop_down\": crop_down,\n",
    "    \"normalize\": normalize,\n",
    "    \"input_format\": input_format,\n",
    "    \"load_edge\": True,\n",
    "    \"allow_missing_files\": False,\n",
    "    \"edge_width\": 3,\n",
    "    \"load_time\": False,\n",
    "#     \"max_frames\": max_frames,\n",
    "}\n",
    "\n",
    "train_data_args7 = {\n",
    "    \"data_path\": \"dataset/train/20201112_131702_aug\",\n",
    "    \"crop_down\": crop_down,\n",
    "    \"normalize\": normalize,\n",
    "    \"input_format\": input_format,\n",
    "    \"load_edge\": True,\n",
    "    \"allow_missing_files\": False,\n",
    "    \"edge_width\": 3,\n",
    "    \"load_time\": False,\n",
    "#     \"max_frames\": max_frames,\n",
    "}\n",
    "\n",
    "train_data_args8 = {\n",
    "    \"data_path\": \"dataset/train/20201112_140127_aug\",\n",
    "    \"crop_down\": crop_down,\n",
    "    \"normalize\": normalize,\n",
    "    \"input_format\": input_format,\n",
    "    \"load_edge\": True,\n",
    "    \"allow_missing_files\": False,\n",
    "    \"edge_width\": 3,\n",
    "    \"load_time\": False,\n",
    "#     \"max_frames\": max_frames,\n",
    "}\n",
    "\n",
    "train_data_args9 = {\n",
    "    \"data_path\": \"dataset/train/20201112_140726_aug\",\n",
    "    \"crop_down\": crop_down,\n",
    "    \"normalize\": normalize,\n",
    "    \"input_format\": input_format,\n",
    "    \"load_edge\": True,\n",
    "    \"allow_missing_files\": True,\n",
    "    \"edge_width\": 3,\n",
    "    \"load_time\": False,\n",
    "    \"max_frames\": 1054,\n",
    "}\n",
    "\n",
    "# Validation dataset configuration\n",
    "val_data_args0 = {\n",
    "    \"data_path\": \"dataset/val/20210309_124809\", # March capture\n",
    "    \"crop_down\": crop_down,\n",
    "    \"normalize\": normalize,\n",
    "    \"input_format\": input_format,\n",
    "    \"load_edge\": True,\n",
    "    \"allow_missing_files\": False,\n",
    "    \"edge_width\": 3,\n",
    "    \"load_time\": False,\n",
    "    \"max_frames\": max_frames,\n",
    "}\n",
    "\n",
    "val_data_args1 = {\n",
    "    \"data_path\": \"dataset/val/20210309_140259\", # March capture\n",
    "    \"crop_down\": crop_down,\n",
    "    \"normalize\": normalize,\n",
    "    \"input_format\": input_format,\n",
    "    \"load_edge\": True,\n",
    "    \"allow_missing_files\": False,\n",
    "    \"edge_width\": 3,\n",
    "    \"load_time\": False,\n",
    "    \"max_frames\": max_frames,\n",
    "}\n",
    "\n",
    "# Test dataset configuration\n",
    "test_data_args0 = {\n",
    "    \"data_path\": \"dataset/test/20210309_130401\", # March capture\n",
    "    \"crop_down\": False,\n",
    "    \"normalize\": normalize,\n",
    "    \"input_format\": input_format,\n",
    "    \"load_edge\": False,\n",
    "    \"edge_width\": 3,\n",
    "    \"load_time\": False,\n",
    "    \"max_frames\": max_frames,\n",
    "}\n",
    "\n",
    "test_data_args1 = {\n",
    "    \"data_path\": \"dataset/test/20210309_140832\", # March capture\n",
    "    \"crop_down\": False,\n",
    "    \"normalize\": normalize,\n",
    "    \"input_format\": input_format,\n",
    "    \"load_edge\": False,\n",
    "    \"edge_width\": 3,\n",
    "    \"load_time\": False,\n",
    "    \"max_frames\": max_frames,\n",
    "}\n",
    "\n",
    "train_data_args = [\n",
    "    train_data_args0,\n",
    "    train_data_args1,\n",
    "    train_data_args2,\n",
    "    train_data_args3,\n",
    "    train_data_args4,\n",
    "    train_data_args5,\n",
    "    train_data_args6,\n",
    "    train_data_args7,\n",
    "    train_data_args8,\n",
    "    train_data_args9,\n",
    "]\n",
    "\n",
    "\n",
    "val_data_args = [\n",
    "    val_data_args0,\n",
    "    val_data_args1,\n",
    "]\n",
    "\n",
    "test_data_args = [\n",
    "    test_data_args0,\n",
    "    test_data_args1,\n",
    "]\n",
    "\n",
    "# Merge train folders\n",
    "train_dataset = ConcatDataset(\n",
    "    [FurrowDataset(arg) for arg in train_data_args]\n",
    ")\n",
    "print(\"-\"*25)\n",
    "print(\"Training datasets:\")\n",
    "print(\"-\"*25)\n",
    "for dataset in train_dataset.datasets:\n",
    "    print(dataset)\n",
    "\n",
    "# Merge validation folders\n",
    "print(\"-\"*25)\n",
    "print(\"Validation datasets:\")\n",
    "print(\"-\"*25)\n",
    "val_dataset = ConcatDataset(\n",
    "    [FurrowDataset(arg) for arg in val_data_args]\n",
    ")\n",
    "for dataset in val_dataset.datasets:\n",
    "    print(dataset)\n",
    "\n",
    "# Merge test folders\n",
    "print(\"-\"*25)\n",
    "print(\"Test datasets:\")\n",
    "print(\"-\"*25)\n",
    "test_dataset = ConcatDataset(\n",
    "    [FurrowDataset(arg) for arg in test_data_args]\n",
    ")\n",
    "for dataset in test_dataset.datasets:\n",
    "    print(dataset)\n",
    "\n",
    "print(f\"Train total: {len(train_dataset)}\")\n",
    "print(f\"Validation total: {len(val_dataset)}\")\n",
    "print(f\"Test total: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model & Optimizer\n",
    "\n",
    "* Use one of the 3 options:\n",
    "  \n",
    "  1.2.1 New Model\n",
    "  \n",
    "  1.2.2 Previously trained on our data\n",
    "  \n",
    "  1.2.3 Original HED\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T13:11:01.328569Z",
     "start_time": "2021-04-20T13:11:01.316164Z"
    }
   },
   "outputs": [],
   "source": [
    "# Input: Configuration for ReduceLROnPlateau\n",
    "scheduler_args = {\n",
    "    \"mode\": \"max\", # Apply scheduling based on metric instead of loss\n",
    "    \"factor\": 0.1,\n",
    "    \"patience\": 1,\n",
    "    \"threshold\": 0.0001,\n",
    "    \"threshold_mode\": \"rel\",\n",
    "    \"cooldown\": 0,\n",
    "    \"min_lr\": 0, \n",
    "    \"eps\": 1e-08, \n",
    "    \"verbose\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create New Model & Optimizer Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T13:11:49.124929Z",
     "start_time": "2021-04-20T13:11:44.007948Z"
    }
   },
   "outputs": [],
   "source": [
    "# Input: Network architecture configuration\n",
    "model_args = {\n",
    "    \"pretrained\": True,          # Pretrained on ImageNet or not\n",
    "    \"freeze\": False,             # Freeze existing weights during training or not\n",
    "    \"input_format\": input_format # darr, rgb, drgb, rgb-darr, rgb-drgb (Configured in 1.1 Dataset)\n",
    "}\n",
    "\n",
    "# Input: Adam optimizer configuration\n",
    "adam_args = {\n",
    "    \"lr\": 5e-04,\n",
    "    \"betas\": (0.9, 0.999),\n",
    "    \"eps\": 1e-08,\n",
    "    \"weight_decay\": 0,\n",
    "    \"amsgrad\": False,\n",
    "}\n",
    "\n",
    "# Input: SGD optimizer configuration\n",
    "sgd_args = {\n",
    "    \"lr\": 0.1, \n",
    "    \"momentum\": 0.9\n",
    "}\n",
    "\n",
    "# Defined optimizers\n",
    "torch_optim = {\n",
    "    \"adam\": (torch.optim.Adam, adam_args),\n",
    "    \"sgd\": (torch.optim.SGD, sgd_args)\n",
    "    #...\n",
    "}\n",
    "\n",
    "start_epoch = 1\n",
    "\n",
    "model = RidgeDetector(model_args)\n",
    "\n",
    "# Input: Pick an optimizer \"adam\" or \"sgd\"\n",
    "optim_choice = 'adam'\n",
    "optim, optim_args = torch_optim[optim_choice]\n",
    "optim = optim(filter(lambda p: p.requires_grad, model.parameters()), **optim_args)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, **scheduler_args)\n",
    "\n",
    "print(model)\n",
    "print(optim)\n",
    "print(\"Schedule LR reduction on plateau:\\n\", scheduler_args, sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Load Stored Model & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T20:35:17.288115Z",
     "start_time": "2021-03-16T20:35:13.202912Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ckpt_path = \"checkpoint/best/best_darr/18_ckpt.pth\"\n",
    "# ckpt_path = \"checkpoint/best/best_rgb/18_ckpt.pth\"\n",
    "# ckpt_path = \"checkpoint/best/best_rgb-darr/8_ckpt.pth\"\n",
    "\n",
    "last_epoch, last_loss, last_acc, model, optim, model_args, optim_choice, optim_args = load_checkpoint(ckpt_path)\n",
    "start_epoch = last_epoch + 1\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, **scheduler_args)\n",
    "\n",
    "print(f\"Model from epoch-{last_epoch} is loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T19:40:21.485697Z",
     "start_time": "2021-03-16T19:40:21.470300Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Manually adjust LR (if needed).\n",
    "print(optim)\n",
    "# for param_group in optim.param_groups:\n",
    "#     param_group['lr'] = 5e-05\n",
    "# print(optim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Original HED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T20:56:55.649451Z",
     "start_time": "2021-03-16T20:56:54.080709Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_args = {\n",
    "    \"pretrained\": False,\n",
    "    \"freeze\": False,\n",
    "    \"input_format\": input_format\n",
    "}\n",
    "weight_map = {\n",
    " 'stage1.0.weight':'moduleVggOne.0.weight',\n",
    " 'stage1.0.bias': 'moduleVggOne.0.bias',\n",
    " 'stage1.2.weight': 'moduleVggOne.2.weight',\n",
    " 'stage1.2.bias': 'moduleVggOne.2.bias',\n",
    " 'sideout1.0.weight': 'moduleScoreOne.weight',\n",
    " 'sideout1.0.bias': 'moduleScoreOne.bias',\n",
    " 'stage2.5.weight':  'moduleVggTwo.1.weight',\n",
    " 'stage2.5.bias':  'moduleVggTwo.1.bias',\n",
    " 'stage2.7.weight':  'moduleVggTwo.3.weight',\n",
    " 'stage2.7.bias':  'moduleVggTwo.3.bias',\n",
    " 'sideout2.0.weight':  'moduleScoreTwo.weight',\n",
    " 'sideout2.0.bias':  'moduleScoreTwo.bias',\n",
    " 'stage3.10.weight':  'moduleVggThr.1.weight',\n",
    " 'stage3.10.bias':  'moduleVggThr.1.bias',\n",
    " 'stage3.12.weight':  'moduleVggThr.3.weight',\n",
    " 'stage3.12.bias':  'moduleVggThr.3.bias',\n",
    " 'stage3.14.weight':  'moduleVggThr.5.weight',\n",
    " 'stage3.14.bias':  'moduleVggThr.5.bias',\n",
    " 'sideout3.0.weight':  'moduleScoreThr.weight',\n",
    " 'sideout3.0.bias':  'moduleScoreThr.bias',\n",
    " 'stage4.17.weight':  'moduleVggFou.1.weight',\n",
    " 'stage4.17.bias':  'moduleVggFou.1.bias',\n",
    " 'stage4.19.weight':  'moduleVggFou.3.weight',\n",
    " 'stage4.19.bias':  'moduleVggFou.3.bias',\n",
    " 'stage4.21.weight':  'moduleVggFou.5.weight',\n",
    " 'stage4.21.bias':  'moduleVggFou.5.bias',\n",
    " 'sideout4.0.weight':  'moduleScoreFou.weight',\n",
    " 'sideout4.0.bias':  'moduleScoreFou.bias',\n",
    " 'stage5.24.weight':  'moduleVggFiv.1.weight',\n",
    " 'stage5.24.bias':  'moduleVggFiv.1.bias',\n",
    " 'stage5.26.weight':  'moduleVggFiv.3.weight',\n",
    " 'stage5.26.bias':  'moduleVggFiv.3.bias',\n",
    " 'stage5.28.weight':  'moduleVggFiv.5.weight',\n",
    " 'stage5.28.bias':  'moduleVggFiv.5.bias',\n",
    " 'sideout5.0.weight':  'moduleScoreFiv.weight',\n",
    " 'sideout5.0.bias':  'moduleScoreFiv.bias',\n",
    " 'fuse.weight': 'moduleCombine.0.weight',\n",
    " 'fuse.bias': 'moduleCombine.0.bias',\n",
    "}\n",
    "\n",
    "start_epoch = 0\n",
    "# Input: Path of HED weights\n",
    "ckpt_path = \"checkpoint/network-bsds500.pytorch\"\n",
    "checkpoint = torch.load(ckpt_path)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = RidgeDetector(model_args)\n",
    "model.to(device)\n",
    "optim_choice = None\n",
    "optim_args = {}\n",
    "\n",
    "state = {}\n",
    "for k1 in model.state_dict().keys():\n",
    "    k2 = weight_map[k1]\n",
    "    state[k1] = checkpoint[k2]\n",
    "    \n",
    "model.load_state_dict(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solver\n",
    "\n",
    "* Configure parameters for logging, loss function and evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T13:13:04.747244Z",
     "start_time": "2021-04-20T13:12:54.496694Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Input: Enter the description for the current experiment:\n",
    "descr = \"Test.\"\n",
    "\n",
    "# Input: Tensorboard logging directory\n",
    "log_folder = 'logs/remove'\n",
    "\n",
    "# Automatically decide run number from the folder.\n",
    "run_id = 0\n",
    "old_runs = filter(lambda x: 'run' in x, os.listdir(log_folder))\n",
    "old_runs = sorted(old_runs, key=lambda x: int(x.split('run')[1]))\n",
    "if old_runs:\n",
    "    run_id = int(old_runs[-1].split('run')[-1]) + 1\n",
    "\n",
    "# Input: Loss function, metric to evaluate performance\n",
    "solver_args = {\n",
    "    \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    \"loss_func\": \"class_balanced_bce\",\n",
    "    \"metric_func\": \"f1\",\n",
    "    \"log_path\": f\"{log_folder}/run{run_id}/\",\n",
    "    \"exp_info\": {\n",
    "        \"descr\": descr,\n",
    "        \"model\": model_args,\n",
    "        \"optim\": {\n",
    "            \"name\": optim_choice, \n",
    "            **optim_args,\n",
    "            **scheduler_args\n",
    "        },\n",
    "        \"train\": train_data_args,\n",
    "        \"val\": val_data_args,\n",
    "        \"test\": test_data_args\n",
    "    }\n",
    "}\n",
    "\n",
    "solver = Solver(solver_args)\n",
    "\n",
    "print(solver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-30T12:30:37.997825Z",
     "start_time": "2020-11-30T12:30:37.979214Z"
    }
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T13:13:10.384972Z",
     "start_time": "2021-04-20T13:13:10.360485Z"
    }
   },
   "outputs": [],
   "source": [
    "# Input: Batch size\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "print(f\"Total train iterations (batch count): {len(train_loader)}\")\n",
    "print(f\"Total validation iterations (batch count): {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training for Hyperparameter Tuning\n",
    "\n",
    "* Set up a short training with possibly less data to tune parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T17:51:23.878975Z",
     "start_time": "2021-03-20T17:18:41.765840Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Input: Configure training\n",
    "train_args = {\n",
    "    \"start_epoch\": start_epoch,\n",
    "    \"end_epoch\": 25,\n",
    "    \"ckpt_path\": \"checkpoint\",\n",
    "    \"ckpt_freq\": 0,     # in epochs\n",
    "    \"train_log_freq\": 1,# in iterations\n",
    "    \"train_vis_freq\": 2,# in iterations\n",
    "    \"val_freq\": 1,      # in epochs\n",
    "    \"val_log_freq\": 1,  # in iterations\n",
    "    \"val_vis_freq\": 1,  # in iterations\n",
    "    \"max_vis\": 5,       # Number of rows in tensorboard image log\n",
    "    \"input_format\": input_format,\n",
    "}\n",
    "solver.train(model, optim, train_loader, val_loader, train_args, scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Training\n",
    "\n",
    "* Train network for longer epochs with entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T13:14:57.251544Z",
     "start_time": "2021-04-20T13:13:46.819394Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Input: Configure training\n",
    "train_args = {\n",
    "    \"start_epoch\": start_epoch,\n",
    "    \"end_epoch\": 20,\n",
    "    \"ckpt_path\": \"checkpoint/remove\",\n",
    "    \"ckpt_freq\": 3,       # in epochs\n",
    "    \"train_log_freq\": 25, # in iterations\n",
    "    \"train_vis_freq\": 50, # in iterations\n",
    "    \"val_freq\": 1,        # in epochs\n",
    "    \"val_log_freq\": 5,    # in iterations\n",
    "    \"val_vis_freq\": 10,   # in iterations\n",
    "    \"max_vis\": 5,         # Number of rows in tensorboard image log\n",
    "    \"input_format\": input_format,\n",
    "}\n",
    "solver.train(model, optim, train_loader, val_loader, train_args, scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T20:55:33.282864Z",
     "start_time": "2021-03-16T20:55:33.258776Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "print(f\"Total validation iterations (batch count): {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T20:57:00.322195Z",
     "start_time": "2021-03-16T20:56:59.120541Z"
    }
   },
   "outputs": [],
   "source": [
    "train_args = {\n",
    "    \"val_freq\": 1,      # in epochs\n",
    "    \"val_log_freq\": 1, # in iterations\n",
    "    \"val_vis_freq\": 1, # in iterations\n",
    "    \"max_vis\": 5,       # Number of rows in tensorboard image log\n",
    "    \"input_format\": input_format,\n",
    "}\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    mean_val_loss, mean_val_score = solver.run_one_epoch(start_epoch, val_loader, model, args=train_args)\n",
    "    message = f\"Average loss/score: {mean_val_loss}/{mean_val_score}\"\n",
    "    solver.writers[\"Train\"].add_text(tag='Description', text_string=message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T16:00:53.865921Z",
     "start_time": "2021-03-01T16:00:53.814Z"
    }
   },
   "outputs": [],
   "source": [
    "test_args = {\n",
    "    \"input_format\": \"darr\",\n",
    "}\n",
    "\n",
    "batch_size = 8\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "print(f\"Total test iterations (batch count): {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T16:00:53.985954Z",
     "start_time": "2021-03-01T16:00:53.967748Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = solver.test(model, test_loader, test_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T16:00:53.987830Z",
     "start_time": "2021-03-01T16:00:53.932Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision.transforms import functional as F\n",
    "from utils.helpers import show_image\n",
    "\n",
    "# TODO: Refactor here\n",
    "def detect(model, image):\n",
    "    model.eval()\n",
    "    X = F.to_tensor(image).unsqueeze(0)\n",
    "    X = F.normalize(X, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    logits = model(X)\n",
    "    logits = logits.squeeze(0)\n",
    "    preds = torch.sigmoid(logits)\n",
    "    print(torch.all(torch.isclose(torch.zeros(1), preds[0])))\n",
    "    print(torch.all(torch.isclose(torch.zeros(1), preds[1])))\n",
    "    print(torch.all(torch.isclose(torch.zeros(1), preds[2])))\n",
    "    print(torch.all(torch.isclose(torch.zeros(1), preds[3])))\n",
    "    print(torch.all(torch.isclose(torch.zeros(1), preds[4])))\n",
    "    print(torch.all(torch.isclose(torch.zeros(1), preds[5])))\n",
    "    pred0 = F.to_pil_image(preds[0])\n",
    "    pred1 = F.to_pil_image(preds[1])\n",
    "    pred2 = F.to_pil_image(preds[2])\n",
    "    pred3 = F.to_pil_image(preds[3])\n",
    "    pred4 = F.to_pil_image(preds[4])\n",
    "    pred5 = F.to_pil_image(preds[5])\n",
    "    pred6 = F.to_pil_image(preds.mean(dim=0, keepdims=True))\n",
    "    return pred0, pred1, pred2, pred3, pred4, pred5, pred6\n",
    "\n",
    "# path = './dataset/20201112_125754/5032_depth.npy'\n",
    "# depth_arr = np.load(path)\n",
    "# depth_arr = np.rint(255 * (depth_arr / depth_arr.max())).astype(np.uint8)\n",
    "# depth_arr = np.stack([depth_arr, depth_arr, depth_arr], axis=-1)\n",
    "# image = Image.open(path)\n",
    "zeros = np.zeros((480,640,3), dtype=np.uint8)\n",
    "detections = detect(model, zeros)\n",
    "for detection in detections:\n",
    "    show_image(detection, cmap=\"gray\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "705px",
    "left": "94px",
    "top": "110px",
    "width": "266px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "711px",
    "left": "1017px",
    "right": "20px",
    "top": "230px",
    "width": "750px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
