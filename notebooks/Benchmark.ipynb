{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T14:58:46.266591Z",
     "start_time": "2021-04-18T14:58:46.263252Z"
    }
   },
   "outputs": [],
   "source": [
    "# Move to the root\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "if os.path.basename(cwd) != \"cv-in-farming\":\n",
    "    os.chdir(\"../\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T14:58:46.992933Z",
     "start_time": "2021-04-18T14:58:46.516854Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "from time import time\n",
    "\n",
    "import cv2\n",
    "from matplotlib.animation import FFMpegWriter\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T14:58:49.038285Z",
     "start_time": "2021-04-18T14:58:48.662897Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from src.dataloader import FurrowDataset\n",
    "from src.image_processing import convert_grayscale, apply_gaussian_blur, apply_otsu_threshold, apply_canny, apply_template_matching\n",
    "from src.model import RidgeDetector\n",
    "from src.solver import load_checkpoint, revert_input_transforms\n",
    "from utils.helpers import create_template, coord_to_mask, overlay_coord, overlay_mask, show_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T14:27:04.629231Z",
     "start_time": "2021-04-18T14:27:04.504897Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "normalize = True\n",
    "input_format = 'darr'\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[Info]: HED will run on: {device}\\n\")\n",
    "\n",
    "# Input: Folder to load for benchmarking different methods\n",
    "folder = \"dataset/val/20210309_124809\" # Or, \"dataset/val/20210309_140259\"\n",
    "\n",
    "dataset_args = {\n",
    "    \"data_path\": folder,\n",
    "    \"crop_down\": False,\n",
    "    \"normalize\": normalize,\n",
    "    \"input_format\": input_format,\n",
    "    \"load_edge\": False,\n",
    "    \"edge_width\": 3,\n",
    "    \"load_time\": False,\n",
    "    \"max_frames\": 5 #1000\n",
    "}\n",
    "\n",
    "dataset = FurrowDataset(dataset_args)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T14:27:06.855236Z",
     "start_time": "2021-04-18T14:27:06.839583Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create folder structure\n",
    "\n",
    "try: os.mkdir(\"benchmark\")\n",
    "except FileExistsError: pass\n",
    "\n",
    "try: os.mkdir(\"benchmark/canny\")\n",
    "except FileExistsError: pass\n",
    "\n",
    "try: os.mkdir(\"benchmark/hed\")\n",
    "except FileExistsError: pass\n",
    "\n",
    "try: os.mkdir(\"benchmark/hed_original\")\n",
    "except FileExistsError: pass\n",
    "\n",
    "try: os.mkdir(\"benchmark/otsu\")\n",
    "except FileExistsError: pass\n",
    "\n",
    "try: os.mkdir(\"benchmark/otsu_canny\")\n",
    "except FileExistsError: pass\n",
    "\n",
    "try: os.mkdir(\"benchmark/template_matching\")\n",
    "except FileExistsError: pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure & Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T14:27:11.763322Z",
     "start_time": "2021-04-18T14:27:11.748964Z"
    }
   },
   "outputs": [],
   "source": [
    "# Input: Pick a frame\n",
    "frame_idx = 2 # 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Canny Edge Detector\n",
    "\n",
    "* Input: RGB\n",
    "* Grayscale + Gaussian Blur + Canny Edge Detector\n",
    "* Output: 1-Channel Binary Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T14:27:38.589352Z",
     "start_time": "2021-04-18T14:27:38.571980Z"
    }
   },
   "outputs": [],
   "source": [
    "canny_config = {\n",
    "    \"visualize\": True,\n",
    "    \"grayscale\": False,\n",
    "    \"ksize\": (15,)*2, \n",
    "    \"sigmaX\": 30,\n",
    "    \"dynamic_thresh\": False,\n",
    "    \"threshold1\": 5000, \n",
    "    \"threshold2\": 11000, \n",
    "    \"apertureSize\": 7,\n",
    "}\n",
    "\n",
    "def canny_edge_detector(image, config):\n",
    "    if config[\"grayscale\"]:\n",
    "        image = convert_grayscale(image, config[\"visualize\"])\n",
    "    \n",
    "    image = apply_gaussian_blur(image, config[\"visualize\"], \n",
    "                                ksize=config[\"ksize\"], \n",
    "                                sigmaX=config[\"sigmaX\"])\n",
    "    \n",
    "    if config[\"dynamic_thresh\"]:\n",
    "        _, threshold2 = apply_otsu_threshold(image, visualize=False)\n",
    "        config[\"threshold2\"] = threshold2\n",
    "        config[\"threshold1\"] = threshold2 // 2\n",
    "    \n",
    "    image = apply_canny(image, config[\"visualize\"], \n",
    "                        threshold1=config[\"threshold1\"],\n",
    "                        threshold2=config[\"threshold2\"],\n",
    "                        apertureSize=config[\"apertureSize\"])\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T14:27:40.296312Z",
     "start_time": "2021-04-18T14:27:39.718811Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = \"benchmark/canny/single_frame\"\n",
    "\n",
    "try: os.mkdir(path)\n",
    "except FileExistsError: pass\n",
    "\n",
    "item = dataset.get_frame_files(frame_idx, \n",
    "                               load_darr=True,\n",
    "                               load_rgb=True,\n",
    "                               load_drgb=False,\n",
    "                               load_edge=True,\n",
    "                               load_time=False)\n",
    "\n",
    "frame_id = item['frame_id']\n",
    "print(f\"Fetching frame: {frame_idx} <-> {frame_id}\")\n",
    "rgb_img = np.array(item['rgb_img'])\n",
    "\n",
    "result = canny_edge_detector(rgb_img, canny_config) / 255\n",
    "\n",
    "overlaid = overlay_mask(rgb_img, result)\n",
    "show_image(overlaid, cmap='gray', ticks=False)\n",
    "\n",
    "cv2.imwrite(os.path.join(path, f\"{frame_id}_overlay.png\"), overlaid*255)\n",
    "np.save(os.path.join(path, f\"{frame_id}_edge_pts.npy\"), result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T14:27:53.390547Z",
     "start_time": "2021-04-18T14:27:51.380874Z"
    }
   },
   "outputs": [],
   "source": [
    "path = f\"benchmark/canny/{dataset.folder_id}\"\n",
    "\n",
    "try: os.mkdir(path)\n",
    "except FileExistsError: pass\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    item = dataset.get_frame_files(i,\n",
    "                                  load_darr=False,\n",
    "                                  load_rgb=True,\n",
    "                                  load_drgb=False,\n",
    "                                  load_edge=True,\n",
    "                                  load_time=False)\n",
    "\n",
    "    rgb_img = np.array(item['rgb_img'])\n",
    "    frame_id = item['frame_id']\n",
    "\n",
    "    result = canny_edge_detector(rgb_img, canny_config) / 255\n",
    "\n",
    "    overlaid = overlay_mask(rgb_img, result)\n",
    "\n",
    "    cv2.imwrite(os.path.join(path, f\"{frame_id}_overlay.png\"), overlaid*255)\n",
    "    np.save(os.path.join(path, f\"{frame_id}_edge_pts.npy\"), result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otsu Thresholding\n",
    "\n",
    "* Input: RGB\n",
    "* Apply Grayscale + Gaussian Blur + Otsu Thresholding\n",
    "* Output: 1-Channel Binary Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T14:28:03.571002Z",
     "start_time": "2021-04-18T14:28:03.554134Z"
    }
   },
   "outputs": [],
   "source": [
    "otsu_config = {\n",
    "    \"visualize\": True,\n",
    "    \"ksize\": (3,3),\n",
    "    \"sigmaX\": 10,\n",
    "}\n",
    "def otsu_thresholding(image, config):\n",
    "    image = convert_grayscale(image, config[\"visualize\"])\n",
    "    image = apply_gaussian_blur(image, config[\"visualize\"], \n",
    "                                ksize=config[\"ksize\"], \n",
    "                                sigmaX=config[\"sigmaX\"])\n",
    "    image, _ = apply_otsu_threshold(image, config[\"visualize\"])\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T14:28:10.564534Z",
     "start_time": "2021-04-18T14:28:09.785190Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = \"benchmark/otsu/single_frame\"\n",
    "\n",
    "try: os.mkdir(path)\n",
    "except FileExistsError: pass\n",
    "\n",
    "item = dataset.get_frame_files(frame_idx, \n",
    "                               load_darr=True,\n",
    "                               load_rgb=True,\n",
    "                               load_drgb=False,\n",
    "                               load_edge=True,\n",
    "                               load_time=False)\n",
    "\n",
    "frame_id = item['frame_id']\n",
    "print(f\"Fetching frame: {frame_idx} <-> {frame_id}\")\n",
    "rgb_img = np.array(item['rgb_img'])\n",
    "\n",
    "result = otsu_thresholding(rgb_img, otsu_config) / 255\n",
    "\n",
    "overlaid = overlay_mask(rgb_img, result)\n",
    "show_image(overlaid, cmap='gray', ticks=False)\n",
    "\n",
    "cv2.imwrite(os.path.join(path, f\"{frame_id}_overlay.png\"), overlaid*255)\n",
    "np.save(os.path.join(path, f\"{frame_id}_edge_pts.npy\"), result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T14:28:21.298929Z",
     "start_time": "2021-04-18T14:28:18.407557Z"
    }
   },
   "outputs": [],
   "source": [
    "path = f\"benchmark/otsu/{dataset.folder_id}\"\n",
    "\n",
    "try: os.mkdir(path)\n",
    "except FileExistsError: pass\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    item = dataset.get_frame_files(i,\n",
    "                                  load_darr=False,\n",
    "                                  load_rgb=True,\n",
    "                                  load_drgb=False,\n",
    "                                  load_edge=True,\n",
    "                                  load_time=False)\n",
    "\n",
    "    rgb_img = np.array(item['rgb_img'])\n",
    "    frame_id = item['frame_id']\n",
    "\n",
    "    result = otsu_thresholding(rgb_img, otsu_config) / 255\n",
    "\n",
    "    overlaid = overlay_mask(rgb_img, result)\n",
    "\n",
    "    cv2.imwrite(os.path.join(path, f\"{frame_id}_overlay.png\"), overlaid*255)\n",
    "    np.save(os.path.join(path, f\"{frame_id}_edge_pts.npy\"), result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otsu + Canny\n",
    "\n",
    "* Input: RGB\n",
    "* Apply Grayscale + Gaussian Blur + Otsu Thresholding + Canny Edge Detector\n",
    "* Output: 1-Channel Binary Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T14:28:39.435879Z",
     "start_time": "2021-04-18T14:28:39.417861Z"
    }
   },
   "outputs": [],
   "source": [
    "otsu_canny_config = {\n",
    "    \"visualize\": True,\n",
    "    \"ksize\": (11,)*2,\n",
    "    \"sigmaX\": 22,\n",
    "    \"apertureSize\": 5,\n",
    "}\n",
    "def otsu_canny_edge_detector(image, config):\n",
    "    image = convert_grayscale(image, config[\"visualize\"])\n",
    "    image = apply_gaussian_blur(image, config[\"visualize\"], \n",
    "                                ksize=config[\"ksize\"], \n",
    "                                sigmaX=config[\"sigmaX\"])\n",
    "    image, threshold2 = apply_otsu_threshold(image, config[\"visualize\"])\n",
    "    image = apply_canny(image, config[\"visualize\"], \n",
    "                    threshold1=threshold2/2,\n",
    "                    threshold2=threshold2,\n",
    "                    apertureSize=config[\"apertureSize\"])\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T14:28:41.246167Z",
     "start_time": "2021-04-18T14:28:40.300876Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = \"benchmark/otsu_canny/single_frame\"\n",
    "\n",
    "try: os.mkdir(path)\n",
    "except FileExistsError: pass\n",
    "\n",
    "item = dataset.get_frame_files(frame_idx, \n",
    "                              load_darr=True,\n",
    "                              load_rgb=True,\n",
    "                              load_drgb=False,\n",
    "                              load_edge=True,\n",
    "                              load_time=False)\n",
    "\n",
    "frame_id = item['frame_id']\n",
    "print(f\"Fetching frame: {frame_idx} <-> {frame_id}\")\n",
    "rgb_img = np.array(item['rgb_img'])\n",
    "\n",
    "result = otsu_canny_edge_detector(rgb_img, otsu_canny_config) / 255\n",
    "\n",
    "overlaid = overlay_mask(rgb_img, result)\n",
    "show_image(overlaid, cmap='gray', ticks=False)\n",
    "\n",
    "cv2.imwrite(os.path.join(path, f\"{frame_id}_overlay.png\"), overlaid*255)\n",
    "np.save(os.path.join(path, f\"{frame_id}_edge_pts.npy\"), result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T14:29:00.156164Z",
     "start_time": "2021-04-18T14:28:56.562256Z"
    }
   },
   "outputs": [],
   "source": [
    "path = f\"benchmark/otsu_canny/{dataset.folder_id}\"\n",
    "\n",
    "try: os.mkdir(path)\n",
    "except FileExistsError: pass\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    item = dataset.get_frame_files(i,\n",
    "                                  load_darr=False,\n",
    "                                  load_rgb=True,\n",
    "                                  load_drgb=False,\n",
    "                                  load_edge=True,\n",
    "                                  load_time=False)\n",
    "\n",
    "    rgb_img = np.array(item['rgb_img'])\n",
    "    frame_id = item['frame_id']\n",
    "\n",
    "    result = otsu_canny_edge_detector(rgb_img, otsu_canny_config) / 255\n",
    "\n",
    "    overlaid = overlay_mask(rgb_img, result)\n",
    "\n",
    "    cv2.imwrite(os.path.join(path, f\"{frame_id}_overlay.png\"), overlaid*255)\n",
    "    np.save(os.path.join(path, f\"{frame_id}_edge_pts.npy\"), result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Template Matching\n",
    "\n",
    "* Input: Depth Array\n",
    "* Apply Template Matching + RANSAC + Curve Fitting\n",
    "* Output: Coordinates for Edge in the Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T14:29:10.432114Z",
     "start_time": "2021-04-18T14:29:10.415087Z"
    }
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"template\": {\n",
    "        \"size\": 30,\n",
    "        \"position\": 1,\n",
    "    },\n",
    "    \"matching\": {\n",
    "        \"start_depth\": 0.92,  # Given in depth-scale\n",
    "        \"contour_width\": 25, # Given in y-scale\n",
    "        \"y_step\": 5,         # Given in y-scale\n",
    "        \"n_contours\": 1000,\n",
    "        \"ransac_thresh\": 30, #15\n",
    "        \"score_thresh\": None,\n",
    "        \"roi\": [None,None,250,None], # min_y:max_y, min_x:max_x\n",
    "        \"fit_type\": \"curve\",\n",
    "        \"verbose\": 0\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T14:29:12.096469Z",
     "start_time": "2021-04-18T14:29:11.516875Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = \"benchmark/template_matching/single_frame\"\n",
    "\n",
    "try: os.mkdir(path)\n",
    "except FileExistsError: pass\n",
    "\n",
    "item = dataset.get_frame_files(frame_idx, \n",
    "                              load_darr=True,\n",
    "                              load_rgb=True,\n",
    "                              load_drgb=False,\n",
    "                              load_edge=True,\n",
    "                              load_time=False)\n",
    "\n",
    "frame_id = item['frame_id']\n",
    "print(f\"Fetching frame: {frame_idx} <-> {frame_id}\")\n",
    "rgb_img = np.array(item['rgb_img'])\n",
    "depth_arr = np.array(item['depth_arr'])\n",
    "\n",
    "# Create a template to find corners\n",
    "template = create_template(**parameters[\"template\"])\n",
    "\n",
    "# Fit a curve (2nd degree polynomial) to inlier detections\n",
    "edge_pixels, inliers, outliers = apply_template_matching(depth_arr, template, **parameters[\"matching\"])\n",
    "\n",
    "overlaid = overlay_coord(rgb_img, edge_pixels, thickness=2)\n",
    "show_image(overlaid, cmap='gray', ticks=False)\n",
    "\n",
    "cv2.imwrite(os.path.join(path, f\"{frame_id}_overlay.png\"), overlaid)\n",
    "np.save(os.path.join(path, f\"{frame_id}_edge_pts.npy\"), edge_pixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T14:29:28.605327Z",
     "start_time": "2021-04-18T14:29:26.413537Z"
    }
   },
   "outputs": [],
   "source": [
    "path = f\"benchmark/template_matching/{dataset.folder_id}\"\n",
    "\n",
    "try: os.mkdir(path)\n",
    "except FileExistsError: pass\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    item = dataset.get_frame_files(i, \n",
    "                                   load_darr=True,\n",
    "                                   load_rgb=True,\n",
    "                                   load_drgb=False,\n",
    "                                   load_edge=False,\n",
    "                                   load_time=False)\n",
    "\n",
    "    frame_id = item['frame_id']\n",
    "    rgb_img = np.array(item['rgb_img'])\n",
    "    depth_arr = item['depth_arr']\n",
    "\n",
    "    # Create a template to find corners\n",
    "    template = create_template(**parameters[\"template\"])\n",
    "\n",
    "    # Fit a curve (2nd degree polynomial) to inlier detections\n",
    "    edge_pixels, inliers, outliers = apply_template_matching(depth_arr, template, **parameters[\"matching\"])\n",
    "\n",
    "    overlaid = overlay_coord(rgb_img, edge_pixels, thickness=2)\n",
    "    cv2.imwrite(os.path.join(path, f\"{frame_id}_overlay.png\"), overlaid)\n",
    "    np.save(os.path.join(path, f\"{frame_id}_edge_pts.npy\"), edge_pixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HED (Original)\n",
    "\n",
    "* Input: Depth Array or RGB\n",
    "* Forward pass through network\n",
    "* Output: 6-Channel Edge Score Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T14:29:45.411994Z",
     "start_time": "2021-04-18T14:29:45.323499Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset has to be re-initialized because original HED takes RGB input.\n",
    "_dataset_args = {**dataset_args}\n",
    "_dataset_args['input_format'] = 'rgb'\n",
    "\n",
    "_dataset = FurrowDataset(_dataset_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T14:29:58.224122Z",
     "start_time": "2021-04-18T14:29:55.568527Z"
    },
    "code_folding": [
     5
    ]
   },
   "outputs": [],
   "source": [
    "model_args = {\n",
    "    \"pretrained\": False,\n",
    "    \"freeze\": False,\n",
    "    \"input_format\": input_format\n",
    "}\n",
    "weight_map = {\n",
    " 'stage1.0.weight':'moduleVggOne.0.weight',\n",
    " 'stage1.0.bias': 'moduleVggOne.0.bias',\n",
    " 'stage1.2.weight': 'moduleVggOne.2.weight',\n",
    " 'stage1.2.bias': 'moduleVggOne.2.bias',\n",
    " 'sideout1.0.weight': 'moduleScoreOne.weight',\n",
    " 'sideout1.0.bias': 'moduleScoreOne.bias',\n",
    " 'stage2.5.weight':  'moduleVggTwo.1.weight',\n",
    " 'stage2.5.bias':  'moduleVggTwo.1.bias',\n",
    " 'stage2.7.weight':  'moduleVggTwo.3.weight',\n",
    " 'stage2.7.bias':  'moduleVggTwo.3.bias',\n",
    " 'sideout2.0.weight':  'moduleScoreTwo.weight',\n",
    " 'sideout2.0.bias':  'moduleScoreTwo.bias',\n",
    " 'stage3.10.weight':  'moduleVggThr.1.weight',\n",
    " 'stage3.10.bias':  'moduleVggThr.1.bias',\n",
    " 'stage3.12.weight':  'moduleVggThr.3.weight',\n",
    " 'stage3.12.bias':  'moduleVggThr.3.bias',\n",
    " 'stage3.14.weight':  'moduleVggThr.5.weight',\n",
    " 'stage3.14.bias':  'moduleVggThr.5.bias',\n",
    " 'sideout3.0.weight':  'moduleScoreThr.weight',\n",
    " 'sideout3.0.bias':  'moduleScoreThr.bias',\n",
    " 'stage4.17.weight':  'moduleVggFou.1.weight',\n",
    " 'stage4.17.bias':  'moduleVggFou.1.bias',\n",
    " 'stage4.19.weight':  'moduleVggFou.3.weight',\n",
    " 'stage4.19.bias':  'moduleVggFou.3.bias',\n",
    " 'stage4.21.weight':  'moduleVggFou.5.weight',\n",
    " 'stage4.21.bias':  'moduleVggFou.5.bias',\n",
    " 'sideout4.0.weight':  'moduleScoreFou.weight',\n",
    " 'sideout4.0.bias':  'moduleScoreFou.bias',\n",
    " 'stage5.24.weight':  'moduleVggFiv.1.weight',\n",
    " 'stage5.24.bias':  'moduleVggFiv.1.bias',\n",
    " 'stage5.26.weight':  'moduleVggFiv.3.weight',\n",
    " 'stage5.26.bias':  'moduleVggFiv.3.bias',\n",
    " 'stage5.28.weight':  'moduleVggFiv.5.weight',\n",
    " 'stage5.28.bias':  'moduleVggFiv.5.bias',\n",
    " 'sideout5.0.weight':  'moduleScoreFiv.weight',\n",
    " 'sideout5.0.bias':  'moduleScoreFiv.bias',\n",
    " 'fuse.weight': 'moduleCombine.0.weight',\n",
    " 'fuse.bias': 'moduleCombine.0.bias',\n",
    "}\n",
    "\n",
    "ckpt_path = \"checkpoint/network-bsds500.pytorch\" # Downloadable from: http://content.sniklaus.com/github/pytorch-hed/network-bsds500.pytorch\n",
    "checkpoint = torch.load(ckpt_path)\n",
    "\n",
    "model = RidgeDetector(model_args)\n",
    "model.to(device)\n",
    "optim_choice = None\n",
    "optim_args = {}\n",
    "\n",
    "state = {}\n",
    "for k1 in model.state_dict().keys():\n",
    "    k2 = weight_map[k1]\n",
    "    state[k1] = checkpoint[k2]\n",
    "    \n",
    "model.load_state_dict(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T14:30:09.595309Z",
     "start_time": "2021-04-18T14:30:08.202365Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path = \"benchmark/hed_original/single_frame\"\n",
    "\n",
    "try: os.mkdir(path)\n",
    "except FileExistsError: pass\n",
    "\n",
    "item = _dataset.get_frame_files(frame_idx, \n",
    "                               load_darr=False,\n",
    "                               load_rgb=True,\n",
    "                               load_drgb=False,\n",
    "                               load_edge=False,\n",
    "                               load_time=False)\n",
    "rgb_img = np.array(item['rgb_img'])\n",
    "\n",
    "item = _dataset.__getitem__(frame_idx)\n",
    "norm_img = item['input'].unsqueeze(0).to(device)\n",
    "\n",
    "model.eval()\n",
    "results = None\n",
    "with torch.no_grad():\n",
    "    results = model(norm_img)\n",
    "    results = torch.sigmoid(results)\n",
    "\n",
    "for result in results[0]:\n",
    "    mask = result.cpu().numpy()\n",
    "    overlaid = overlay_mask(rgb_img, mask)\n",
    "    show_image(overlaid, ticks=False)\n",
    "\n",
    "mask = results[0,4].cpu().numpy()\n",
    "overlaid = overlay_mask(rgb_img, mask)\n",
    "cv2.imwrite(os.path.join(path, f\"{frame_id}_overlay.png\"), overlaid*255)\n",
    "np.save(os.path.join(path, f\"{frame_id}_edge_pts.npy\"), mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T14:30:21.074083Z",
     "start_time": "2021-04-18T14:30:20.593277Z"
    }
   },
   "outputs": [],
   "source": [
    "path = f\"benchmark/hed_original/{_dataset.folder_id}\"\n",
    "\n",
    "try: os.mkdir(path)\n",
    "except FileExistsError: pass\n",
    "\n",
    "def detect_per_sample(model, sample, device):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        X = sample['input'].to(device)\n",
    "        logits = model(X)\n",
    "        preds = torch.sigmoid(logits)\n",
    "        mask = preds[:,4:5,:,:][0,0]\n",
    "        return mask.cpu().numpy()\n",
    "\n",
    "loader = DataLoader(_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "t1 = time()\n",
    "\n",
    "model.to(device)\n",
    "td = 0\n",
    "for i, sample in enumerate(loader):\n",
    "    item = _dataset.get_frame_files(i,\n",
    "                                   load_darr=False,\n",
    "                                   load_rgb=True,\n",
    "                                   load_drgb=False,\n",
    "                                   load_edge=False,\n",
    "                                   load_time=False)\n",
    "    rgb_img = np.array(item['rgb_img'])\n",
    "    frame_id = item['frame_id']\n",
    "    td1 = time()\n",
    "    mask = detect_per_sample(model, sample, device)\n",
    "    td2 = time()\n",
    "    td += (td2 - td1)\n",
    "    \n",
    "    # Store result\n",
    "    overlaid = overlay_mask(rgb_img, mask)\n",
    "    cv2.imwrite(os.path.join(path, f\"{frame_id}_overlay.png\"), overlaid*255)\n",
    "    np.save(os.path.join(path, f\"{frame_id}_edge_pts.npy\"), mask)\n",
    "\n",
    "t2 = time()\n",
    "print(\"Total duration:\", t2-t1)\n",
    "print(\"Avg duration:\", (t2-t1)/len(_dataset))\n",
    "print(\"Total detection duration:\", td)\n",
    "print(\"Avg detection duration:\", td/len(_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HED (Fine-tuned)\n",
    "\n",
    "* Input: Depth Array or RGB or RGB + Depth Array\n",
    "* Forward pass through network\n",
    "* Output: 6-Channel Edge Score Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T14:30:32.186730Z",
     "start_time": "2021-04-18T14:30:31.351812Z"
    }
   },
   "outputs": [],
   "source": [
    "ckpt_path = \"checkpoint/darr/18_ckpt.pth\"\n",
    "# ckpt_path = \"checkpoint/rgb/18_ckpt.pth\"\n",
    "# ckpt_path = \"checkpoint/rgb-darr/18_ckpt.pth\"\n",
    "\n",
    "last_epoch, _, _, model, _, _, _, _ = load_checkpoint(ckpt_path)\n",
    "\n",
    "print(f\"Model from epoch-{last_epoch} is loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T14:30:35.130394Z",
     "start_time": "2021-04-18T14:30:33.857647Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = \"benchmark/hed/single_frame\"\n",
    "\n",
    "try: os.mkdir(path)\n",
    "except FileExistsError: pass\n",
    "\n",
    "item = dataset.get_frame_files(frame_idx, \n",
    "                              load_darr=False,\n",
    "                              load_rgb=True,\n",
    "                              load_drgb=False,\n",
    "                              load_edge=False,\n",
    "                              load_time=False)\n",
    "rgb_img = np.array(item['rgb_img'])\n",
    "\n",
    "item = dataset.__getitem__(frame_idx)\n",
    "norm_img = item['input'].unsqueeze(0).to(device)\n",
    "\n",
    "model.eval()\n",
    "results = None\n",
    "with torch.no_grad():\n",
    "    results = model(norm_img)\n",
    "    results = torch.sigmoid(results)\n",
    "\n",
    "for result in results[0]:\n",
    "    mask = result\n",
    "    overlaid = overlay_mask(rgb_img, mask.cpu())\n",
    "    show_image(overlaid, ticks=False)\n",
    "    \n",
    "mask = results[0,-1].cpu().numpy()\n",
    "overlaid = overlay_mask(rgb_img, mask)\n",
    "cv2.imwrite(os.path.join(path, f\"{frame_id}_overlay.png\"), overlaid*255)\n",
    "np.save(os.path.join(path, f\"{frame_id}_edge_pts.npy\"), mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T14:30:46.334986Z",
     "start_time": "2021-04-18T14:30:45.914480Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path = f\"benchmark/hed/{dataset.folder_id}\"\n",
    "\n",
    "try: os.mkdir(path)\n",
    "except FileExistsError: pass\n",
    "\n",
    "def detect_per_sample(model, sample, device):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        X = sample['input'].to(device)\n",
    "        logits = model(X)\n",
    "        preds = torch.sigmoid(logits)\n",
    "        mask = preds[:,5:6,:,:][0,0]\n",
    "        return mask.cpu().numpy()\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "t1 = time()\n",
    "\n",
    "model.to(device)\n",
    "td = 0\n",
    "for i, sample in enumerate(loader):\n",
    "    item = dataset.get_frame_files(i,\n",
    "                                   load_darr=False,\n",
    "                                   load_rgb=True,\n",
    "                                   load_drgb=False,\n",
    "                                   load_edge=False,\n",
    "                                   load_time=False)\n",
    "    rgb_img = np.array(item['rgb_img'])\n",
    "    frame_id = item['frame_id']\n",
    "    td1 = time()\n",
    "    mask = detect_per_sample(model, sample, device)\n",
    "    td2 = time()\n",
    "    td += (td2 - td1)\n",
    "    \n",
    "    # Store result\n",
    "    overlaid = overlay_mask(rgb_img, mask)\n",
    "    cv2.imwrite(os.path.join(path, f\"{frame_id}_overlay.png\"), overlaid*255)\n",
    "    np.save(os.path.join(path, f\"{frame_id}_edge_pts.npy\"), mask)\n",
    "\n",
    "t2 = time()\n",
    "print(\"Total duration:\", t2-t1)\n",
    "print(\"Avg duration:\", (t2-t1)/len(dataset))\n",
    "print(\"Total detection duration:\", td)\n",
    "print(\"Avg detection duration:\", td/len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Samples from November and March Captures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T14:59:09.622808Z",
     "start_time": "2021-04-18T14:59:08.596959Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_depth(path):\n",
    "    depth_arr = np.load(path)\n",
    "    depth_arr = np.rint(255 * (depth_arr / depth_arr.max()))\n",
    "    depth_arr = np.clip(depth_arr * 7, a_min=0, a_max=255).astype(np.uint8)\n",
    "    return depth_arr\n",
    "\n",
    "fig = plt.figure(figsize=[15,15])\n",
    "gs = gridspec.GridSpec(1, 4, wspace=0, hspace=0)\n",
    "\n",
    "path = \"notebooks/Benchmark Content/Samples from November and March\"\n",
    "\n",
    "grid1 = np.array([\n",
    "    [np.array(Image.open(f\"{path}/November/3900_rgb.png\")), np.array(Image.open(f\"{path}/November/26600_rgb.png\"))],\n",
    "    [np.stack([read_depth(f\"{path}/November/3900_depth.npy\")]*3, axis=-1), np.stack([read_depth(f\"{path}/November/26600_depth.npy\")]*3, axis=-1)]\n",
    "])\n",
    "\n",
    "grid2 = np.array([\n",
    "    [np.array(Image.open(f\"{path}/March/3000_rgb.png\")), np.array(Image.open(f\"{path}/March/5000_rgb.png\"))],\n",
    "    [np.stack([read_depth(f\"{path}/March/3000_depth.npy\")]*3, axis=-1), np.stack([read_depth(f\"{path}/March/5000_depth.npy\")]*3, axis=-1)]\n",
    "])\n",
    "\n",
    "gs1 = gridspec.GridSpec(2, 2)\n",
    "gs1.update(left=0, right=1, hspace=0, wspace=0)\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        ax = fig.add_subplot(gs1[i, j])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_frame_on(False)\n",
    "        ax.imshow(grid1[i, j])\n",
    "        ax.set_aspect(\"auto\")\n",
    "        if i == 0 and j == 0:\n",
    "            ax.text(425, -25, 'Samples from November', size=33)\n",
    "        \n",
    "gs2 = gridspec.GridSpec(2, 2)\n",
    "gs2.update(left=1.02, right=2, hspace=0, wspace=0)\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        ax = fig.add_subplot(gs2[i, j])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_frame_on(False)\n",
    "        ax.imshow(grid2[i, j])\n",
    "        ax.set_aspect(\"auto\")\n",
    "        if i == 0 and j == 0:\n",
    "            ax.text(425, -25, 'Samples from March', size=33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Sample for HED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T14:59:10.261179Z",
     "start_time": "2021-04-18T14:59:09.657082Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "normalize = True\n",
    "input_format = 'rgb'\n",
    "\n",
    "dataset_args = {\n",
    "    \"data_path\": \"dataset/train/20201112_125754\", # New capture\n",
    "    \"crop_down\": False,\n",
    "    \"normalize\": normalize,\n",
    "    \"input_format\": input_format,\n",
    "    \"load_edge\": False,\n",
    "    \"edge_width\": 3,\n",
    "    \"load_time\": False,\n",
    "}\n",
    "dataset = FurrowDataset(dataset_args)\n",
    "\n",
    "frame_idx = 0\n",
    "item = dataset.get_frame_files(frame_idx, \n",
    "                               load_darr=True,\n",
    "                               load_rgb=True,\n",
    "                               load_drgb=False,\n",
    "                               load_edge=True,\n",
    "                               load_time=False)\n",
    "rgb_img = np.array(item['rgb_img'])\n",
    "depth_arr = item['depth_arr']\n",
    "depth_arr = np.rint(255 * (depth_arr / depth_arr.max()))\n",
    "depth_arr = np.clip(depth_arr * 7, a_min=0, a_max=255).astype(np.uint8)\n",
    "edge_pixels = item['edge_pixels']\n",
    "\n",
    "crop = cv2.rectangle(rgb_img.copy(), (120, 80), (520, 480), [255, 0, 0], thickness=2)\n",
    "rgb_input = apply_gaussian_blur(rgb_img, False, ksize=(7,7), sigmaX=20)\n",
    "rgb_input[80:480,120:520] = crop[80:480,120:520]\n",
    "# show_image(rgb_input, ticks=False)\n",
    "\n",
    "crop = cv2.rectangle(depth_arr.copy(), (120, 80), (520, 480), [255, 0, 0], thickness=2)\n",
    "depth_input = apply_gaussian_blur(depth_arr, False, ksize=(7,7), sigmaX=20)\n",
    "depth_input[80:480,120:520] = crop[80:480,120:520]\n",
    "# show_image(depth_input, ticks=False, cmap=\"gray\")\n",
    "\n",
    "edge_mask = coord_to_mask((480,640), edge_pixels, thickness=2)\n",
    "crop = cv2.rectangle(edge_mask.copy(), (120, 80), (520, 480), 255, thickness=2)\n",
    "edge_img = apply_gaussian_blur(edge_mask, False, ksize=(7,7), sigmaX=20)\n",
    "edge_img[80:480,120:520] = crop[80:480,120:520]\n",
    "# show_image(edge_img, ticks=False, cmap=\"gray\")\n",
    "\n",
    "overlaid = overlay_coord(rgb_img, edge_pixels, thickness=2)\n",
    "crop = cv2.rectangle(overlaid.copy(), (120, 80), (520, 480), [255, 0, 0], thickness=2)\n",
    "overlay_rgb = apply_gaussian_blur(overlaid, False, ksize=(7,7), sigmaX=20)\n",
    "overlay_rgb[80:480,120:520] = crop[80:480,120:520]\n",
    "# show_image(overlay_rgb, ticks=False)\n",
    "\n",
    "overlaid = overlay_coord(depth_arr, edge_pixels, thickness=2)\n",
    "crop = cv2.rectangle(overlaid.copy(), (120, 80), (520, 480), 255, thickness=2)\n",
    "overlay_darr = apply_gaussian_blur(overlaid, False, ksize=(7,7), sigmaX=20)\n",
    "overlay_darr[80:480,120:520] = crop[80:480,120:520]\n",
    "# show_image(overlay_darr, ticks=False, cmap=\"gray\")\n",
    "\n",
    "fig = plt.figure(figsize=[40,8])\n",
    "gs = gridspec.GridSpec(1, 4, wspace=0, hspace=0)\n",
    "imgs = np.array([\n",
    "    rgb_input, np.stack([depth_input]*3, axis=-1), np.stack([edge_img]*3, axis=-1), overlay_rgb\n",
    "])\n",
    "\n",
    "labels = [\"Cropped RGB Input\", \"Cropped Depth Input\", \"Cropped Edge Mask Target\", \"Cropped RGB with Mask Overlaid\"]\n",
    "\n",
    "for i in range(4):\n",
    "    ax = fig.add_subplot(gs[0, i])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_frame_on(False)\n",
    "    ax.imshow(imgs[i])\n",
    "    ax.set_aspect(\"auto\")\n",
    "    ax.xaxis.set_label_position('top') \n",
    "    ax.set_xlabel(labels[i], fontsize=33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Template Matching vs HED on Extreme Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T14:59:10.768604Z",
     "start_time": "2021-04-18T14:59:10.331472Z"
    }
   },
   "outputs": [],
   "source": [
    "tm = \"notebooks/Benchmark Content/Compare HED and TM/Template Matching\"\n",
    "hed = \"notebooks/Benchmark Content/Compare HED and TM/HED (Depth-Only)\"\n",
    "\n",
    "frame_ids = [5906, 8971, 9404]\n",
    "\n",
    "grid = [\n",
    "    [],\n",
    "    [],\n",
    "    []\n",
    "]\n",
    "\n",
    "for i, frame_id in enumerate(frame_ids):\n",
    "    grid[i].append(np.array(Image.open(f\"{hed}/{frame_id}_depth.png\")))\n",
    "    grid[i].append(np.array(Image.open(f\"{tm}/{frame_id}_overlay.png\")))\n",
    "    grid[i].append(np.array(Image.open(f\"{hed}/{frame_id}_overlay.png\")))\n",
    "\n",
    "grid = np.array(grid)\n",
    "nrows, ncols = grid.shape[:2]\n",
    "\n",
    "fig = plt.figure(figsize=[8,8])\n",
    "\n",
    "gs = gridspec.GridSpec(nrows, ncols)\n",
    "gs.update(left=0, right=1, hspace=0, wspace=0)\n",
    "size = 19\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "        ax = fig.add_subplot(gs[i, j])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.imshow(grid[i,j])\n",
    "        ax.set_aspect(\"auto\")\n",
    "        if i == 0 and j == 0:\n",
    "            ax.xaxis.set_label_position('top') \n",
    "            ax.set_xlabel('Input', fontsize=size)\n",
    "        elif i == 0 and j == 1:\n",
    "            ax.xaxis.set_label_position('top') \n",
    "            ax.set_xlabel('Template Matching', fontsize=size)\n",
    "        elif i == 0 and j == 2:\n",
    "            ax.xaxis.set_label_position('top') \n",
    "            ax.set_xlabel('HED', fontsize=size)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Best of Each Method on a Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T14:59:11.421519Z",
     "start_time": "2021-04-18T14:59:11.100864Z"
    }
   },
   "outputs": [],
   "source": [
    "path = \"notebooks/Benchmark Content/Compare Methods\"\n",
    "fig = plt.figure(figsize=[10,10])\n",
    "\n",
    "labels = [[\"Otsu+Canny\", \"Template Matching\"], [\"HED Original\", \"HED Finetuned (Ours)\"]]\n",
    "\n",
    "gs = gridspec.GridSpec(2, 2, wspace=0, hspace=0.1)\n",
    "imgs = np.array([\n",
    "    [np.array(Image.open(f\"{path}/otsu+canny1.png\")), np.array(Image.open(f\"{path}/template_matching1.png\"))],\n",
    "    [np.array(Image.open(f\"{path}/hed_original1.png\")), np.array(Image.open(f\"{path}/hed-darr1.png\"))],\n",
    "])\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        ax = fig.add_subplot(gs[i, j])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_frame_on(False)\n",
    "        ax.imshow(imgs[i, j])\n",
    "        ax.set_aspect(\"auto\")\n",
    "        ax.xaxis.set_label_position('top') \n",
    "        ax.set_xlabel(labels[i][j], fontsize=22)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HED Side Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T14:59:12.319914Z",
     "start_time": "2021-04-18T14:59:11.982264Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "darr = \"notebooks/Benchmark Content/HED-related/Side Outputs/Depth-Only\"\n",
    "rgb = \"notebooks/Benchmark Content/HED-related/Side Outputs/RGB-Only\"\n",
    "rgb_darr = \"notebooks/Benchmark Content/HED-related/Side Outputs/RGB+Depth\"\n",
    "\n",
    "grid = [[], [], []]\n",
    "labels = [\"Side Output (x1)\", \"Side Output (x2)\", \"Side Output (x4)\", \"Side Output (x8)\", \"Side Output (x16)\", \"Side Output Fusion\"]\n",
    "\n",
    "for i in range(1,7):\n",
    "    grid[0].append(np.array(Image.open(f\"{darr}/scale{i}.png\")))\n",
    "#     grid[1].append(np.array(Image.open(f\"{rgb}/scale{i}.png\")))\n",
    "#     grid[2].append(np.array(Image.open(f\"{rgb_darr}/scale{i}.png\")))\n",
    "\n",
    "fig = plt.figure(figsize=[24, 3], frameon=False)\n",
    "# fig.suptitle(\"HED Side Outputs with Different Inputs\", fontsize=30, y=0.94)\n",
    "gs = gridspec.GridSpec(1, 6, wspace=0, hspace=0.05)\n",
    "\n",
    "for i in range(1):\n",
    "    for j in range(6):\n",
    "        ax = fig.add_subplot(gs[i, j])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_frame_on(False)\n",
    "        ax.imshow(grid[i][j])\n",
    "        ax.set_aspect(\"auto\")\n",
    "        ax.xaxis.set_label_position('top')\n",
    "        ax.set_xlabel(labels[j], fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of Data Augmentation on HED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T14:59:13.275383Z",
     "start_time": "2021-04-18T14:59:12.832586Z"
    }
   },
   "outputs": [],
   "source": [
    "path1 = \"notebooks/Benchmark Content/HED-related/Mirror Augmentation/With Mirror/\"\n",
    "path2 = \"notebooks/Benchmark Content/HED-related/Mirror Augmentation/Without Mirror/\"\n",
    "fig = plt.figure(figsize=[15,12], frameon=False)\n",
    "\n",
    "labels = [\"with mirror augmentation\", \"without mirror augmentation\"]\n",
    "\n",
    "gs = gridspec.GridSpec(2, 2, wspace=0, hspace=0)\n",
    "imgs = np.array([\n",
    "    [np.array(Image.open(f\"{path1}/front-1.png\")), np.array(Image.open(f\"{path2}/front-1.png\"))],\n",
    "    [np.array(Image.open(f\"{path1}/back-2.png\")), np.array(Image.open(f\"{path2}/back-2.png\"))]\n",
    "])\n",
    "\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        ax = fig.add_subplot(gs[i, j])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_frame_on(False)\n",
    "        ax.imshow(imgs[i, j])\n",
    "        ax.set_aspect(\"auto\")\n",
    "        if i == 0:\n",
    "            ax.xaxis.set_label_position('top')\n",
    "            ax.set_xlabel(labels[j], fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Template Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-26T10:40:52.103667Z",
     "start_time": "2021-03-26T10:40:50.613939Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams['animation.ffmpeg_path'] = '/usr/bin/ffmpeg'\n",
    "\n",
    "# Input: Path contains RGB frames and detections\n",
    "frame_path = \"demo/template_matching\"\n",
    "# Input: Output video name\n",
    "video_name = \"demo/Template Matching\"\n",
    "\n",
    "files = os.listdir(frame_path)\n",
    "\n",
    "rgb_im_files = []\n",
    "edge_px_files = []\n",
    "\n",
    "# Filter files wrt their extension\n",
    "for file in files:\n",
    "    if file.endswith(\"rgb.png\"):\n",
    "        rgb_im_files.append(file)\n",
    "    if file.endswith(\"edge_pts.npy\"):\n",
    "        edge_px_files.append(file)\n",
    "\n",
    "rgb_im_files = sorted(rgb_im_files, key=lambda f: int(f.split(\"_\")[0]))\n",
    "edge_px_files = sorted(edge_px_files, key=lambda f: int(f.split(\"_\")[0]))\n",
    "\n",
    "fig = plt.figure()\n",
    "cut = range(1164, 2965)\n",
    "\n",
    "metadata = dict(title='Template Matching Detector')\n",
    "writer = FFMpegWriter(fps=30, metadata=metadata)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "imgh = plt.imshow(np.zeros((480, 640), dtype=np.uint8))\n",
    "\n",
    "with writer.saving(fig, f\"{video_name}.mp4\", 100):\n",
    "    for i in cut:\n",
    "        rgb_im_file = rgb_im_files[i]\n",
    "        edge_px_file = edge_px_files[i]\n",
    "        \n",
    "        frame_idx = rgb_im_file.split(\"_\")[0]\n",
    "\n",
    "        rgb_im_path = os.path.join(frame_path, rgb_im_file)\n",
    "        rgb_img = cv2.imread(rgb_im_path, cv2.IMREAD_COLOR)\n",
    "        \n",
    "        edge_px_path = os.path.join(frame_path, f\"{frame_idx}_edge_pts.npy\")\n",
    "        edge_pixels = np.load(edge_px_path)\n",
    "        overlaid = overlay_coord(rgb_img, edge_pixels, thickness=2)\n",
    "        \n",
    "        imgh.set_data(overlaid)\n",
    "        \n",
    "        writer.grab_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-26T10:45:22.832378Z",
     "start_time": "2021-03-26T10:44:33.911698Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams['animation.ffmpeg_path'] = '/usr/bin/ffmpeg'\n",
    "\n",
    "# Input: Edge overlaid RGB frame path\n",
    "frame_path = \"demo/hed\"\n",
    "# Input: Output video name\n",
    "video_name = \"demo/HED\"\n",
    "\n",
    "files = os.listdir(frame_path)\n",
    "\n",
    "rgb_im_files = []\n",
    "\n",
    "# Filter files wrt their extension\n",
    "for file in files:\n",
    "    if file.endswith(\".png\"):\n",
    "        rgb_im_files.append(file)\n",
    "\n",
    "rgb_im_files = sorted(rgb_im_files, key=lambda f: int(f.split(\"_\")[0]))\n",
    "\n",
    "fig = plt.figure()\n",
    "cut = rgb_im_files\n",
    "\n",
    "metadata = dict(title='HED Detector')\n",
    "writer = FFMpegWriter(fps=30, metadata=metadata)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "imgh = plt.imshow(np.zeros((480, 640), dtype=np.uint8))\n",
    "\n",
    "with writer.saving(fig, f\"{video_name}.mp4\", 100):\n",
    "    for rgb_im_file in cut:\n",
    "        frame_idx = rgb_im_file.split(\"_\")[0]\n",
    "\n",
    "        rgb_im_path = os.path.join(frame_path, rgb_im_file)\n",
    "        rgb_img = cv2.imread(rgb_im_path, cv2.IMREAD_COLOR)\n",
    "        \n",
    "        imgh.set_data(rgb_img)\n",
    "        \n",
    "        writer.grab_frame()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "705px",
    "left": "94px",
    "top": "110px",
    "width": "266px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "711px",
    "left": "1017px",
    "right": "20px",
    "top": "230px",
    "width": "750px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
